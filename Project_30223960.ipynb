{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Problem Statement:\n",
        "For a utility company looking to optimize energy consumption and reduce costs, predicting energy usage accurately is crucial. The client wishes to develop a machine learning model that can forecast energy usage based on various parameters such as temperature, humidity, wind speed, and time of day. By accurately predicting energy demand, the client can optimize resource allocation, schedule maintenance, and improve overall operational efficiency."
      ],
      "metadata": {
        "id": "ZdNcxs-rx0Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. LINEAR REGRESSION"
      ],
      "metadata": {
        "id": "zluSOSyYLi83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np  # For numerical operations\n",
        "import pandas as pd  # For data manipulation\n",
        "from sklearn.model_selection import train_test_split  # To split the dataset\n",
        "from sklearn.linear_model import LinearRegression  # Initiating Linear regression model\n",
        "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error  # For model evaluation\n",
        "\n",
        "\n",
        "\n",
        "# Load the energy dataset\n",
        "from yellowbrick.datasets import load_energy  # Import the dataset loader\n",
        "X, y = load_energy()  # Load features (X) and target variable (y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)  # Split data\n",
        "\n",
        "# Initialize and train the Linear Regression model\n",
        "lr = LinearRegression().fit(X_train, y_train)  # Train model on training data\n",
        "\n",
        "# Print the R² score for training and validation sets\n",
        "print(\"Training score : {:.8f}\".format(lr.score(X_train, y_train)))  # Training R²\n",
        "print(\"Validation score : {:.8f}\".format(lr.score(X_val, y_val)))  # Validation R²\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = lr.predict(X_val)  # Predictions for validation set\n",
        "\n",
        "# Evaluate the model using mean squared error and Mean Absolute error\n",
        "mse = mean_squared_error(y_val, y_pred)  # Calculate MSE\n",
        "lr_mae = mean_absolute_error(y_val, lr.predict(X_val))\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")  # Output MSE\n",
        "print(f\" MAE: {lr_mae:.2f}\")#Output MAE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDcZiYOC4BCT",
        "outputId": "4dc1328a-ba56-4d56-b2dd-61f118063f30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training score : 0.91728565\n",
            "Validation score : 0.91124809\n",
            "Mean Squared Error (MSE): 9.586373592719587\n",
            "Linear Regression MAE: 2.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Random Forrest Regression\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jeAUvefXLtPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor  # Random forest model for regression\n",
        "\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_val, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Initialize the Random Forest Regressor with 100 trees and a fixed random state for reproducibility\n",
        "random_forest_model = RandomForestRegressor( random_state=0)\n",
        "random_forest_model.fit(X_train, y_train)  # Train the model on the training data\n",
        "\n",
        "# Use the trained model to make predictions on the test dataset\n",
        "y_pred = random_forest_model.predict(X_val)\n",
        "\n",
        "# Calculate Mean Squared Error between true and predicted values for test set\n",
        "mse = mean_squared_error(y_test, y_pred)  # Mean squared error\n",
        "rf_mae = mean_absolute_error(y_test, random_forest_model.predict(X_val)) #Mean Absolute error\n",
        "\n",
        "# Calculate the model's performance scores on training and test data for comparison\n",
        "training_score = random_forest_model.score(X_train, y_train)  # R² score on the training set\n",
        "test_score = random_forest_model.score(X_val, y_test)  # R² score on the test set\n",
        "\n",
        "\n",
        "# Output the results\n",
        "# Assuming it should be 'random_forest_model', the corrected lines are:\n",
        "print(\"Training score: {:.8f}\".format(random_forest_model.score(X_train, y_train)))  # Print training R² score\n",
        "print(\"Test score: {:.8f}\".format(random_forest_model.score(X_val, y_test)))  # Print test R² score\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")  # Print the mean squared error\n",
        "print(f\" MAE: {rf_mae:.2f}\")\n",
        "\n",
        "rf_feature_importances = random_forest_model.feature_importances_\n",
        "print(f\"\\nRandom Forest Feature Importances:\")\n",
        "for name, importance in zip(X.columns, rf_feature_importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSAOQUmA46zu",
        "outputId": "60f77c68-93be-4d05-b289-734a3fd8db48"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training score: 0.99969889\n",
            "Test score: 0.99698276\n",
            "Mean Squared Error (MSE): 0.3331825161688284\n",
            " MAE: 0.38\n",
            "\n",
            "Random Forest Feature Importances:\n",
            "relative compactness: 0.4027\n",
            "surface area: 0.1753\n",
            "wall area: 0.0505\n",
            "roof area: 0.1613\n",
            "overall height: 0.1187\n",
            "orientation: 0.0007\n",
            "glazing area: 0.0794\n",
            "glazing area distribution: 0.0114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Decision Trees"
      ],
      "metadata": {
        "id": "hL1qhv9Ww7Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Decision Tree Regressor model from scikit-learn\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "# Initialize the Decision Tree Regressor with a maximum depth of 5\n",
        "decision_tree_regressor = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "# Import utilities for splitting datasets and conducting cross-validation\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Initialize another Decision Tree Regressor with specified max depth and random state for reproducibility\n",
        "tree = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
        "# Fit the decision tree model on the training data\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Print model accuracy (R^2 score) on both training and validation sets\n",
        "print(\"Accuracy on training set: {:.8f}\".format(tree.score(X_train, y_train)))\n",
        "print(\"Accuracy on validation set: {:.8f}\".format(tree.score(X_val, y_val)))\n",
        "\n",
        "# Perform cross-validation to assess model performance, using mean squared error as the scoring metric\n",
        "scores = cross_validate(tree, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
        "\n",
        "# Print the mean cross-validation scores for both training and validation datasets\n",
        "# Note: Scores are negated to convert from negative mean squared error to mean squared error\n",
        "for label_pair in [('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
        "    print('{} = {:.3f}'.format(label_pair[1], -scores[label_pair[0]].mean()))\n",
        "dt_mae = mean_absolute_error(y_val, tree.predict(X_val))#Mean Absolute error\n",
        "\n",
        "# Calculate and print the model's accuracy on training and validation sets\n",
        "tree_training_accuracy = tree.score(X_train, y_train)\n",
        "tree_validation_accuracy = tree.score(X_val, y_val)\n",
        "results_df = pd.DataFrame({\n",
        "    'Training accuracy': [tree_training_accuracy],\n",
        "    'Validation accuracy': [tree_validation_accuracy]\n",
        "}, index=['DT'])\n",
        "print(results_df)\n",
        "\n",
        "# Conduct another round of cross-validation, this time evaluating R^2 scores\n",
        "cv_results = cross_validate(tree, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
        "train_r2 = cv_results['train_score']\n",
        "val_r2 = cv_results['test_score']\n",
        "\n",
        "# Calculate and print the average R^2 scores across the cross-validation folds for both training and validation sets\n",
        "average_train_r2 = np.mean(train_r2)\n",
        "average_val_r2 = np.mean(val_r2)\n",
        "print(f\"Average Training R^2 for DT: {average_train_r2}\")\n",
        "print(f\"Average Validation R^2 for DT: {average_val_r2}\")\n",
        "print(f\"MAE: {dt_mae:.2f}\")#Mean Absoulte error\n",
        "\n",
        "y_pred_dt = tree.predict(X_val)\n",
        "mse_dt = mean_squared_error(y_val, y_pred_dt)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE) for Decision Tree: {mse_dt:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qigf8v6N6Jss",
        "outputId": "f247a2af-895f-449f-e14e-4d150ae5c9d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.99049843\n",
            "Accuracy on validation set: 0.98917684\n",
            "train_score = 0.986\n",
            "validation_score = 1.119\n",
            "    Training accuracy  Validation accuracy\n",
            "DT           0.990498             0.989177\n",
            "Average Training R^2 for DT: 0.9900547524289627\n",
            "Average Validation R^2 for DT: 0.9884104707034591\n",
            "MAE: 0.75\n",
            "Mean Squared Error (MSE) for Decision Tree: 1.16904366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Gradient Boosting Regressor"
      ],
      "metadata": {
        "id": "0RkFrI1dxwcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Gradient Boosting Regressor model from scikit-learn\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "# Import utilities for cross-validation\n",
        "from sklearn.model_selection import cross_validate, cross_val_score\n",
        "# Initialize the Gradient Boosting Regressor with specified max depth and random state for reproducibility\n",
        "gradient_boosting_regressor = GradientBoostingRegressor(max_depth=5)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Initialize a Gradient Boosting Regressor with a maximum depth of 5 and random state for reproducibility\n",
        "gbrt = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
        "# Fit the model on the training data\n",
        "gbrt.fit(X_train, y_train)\n",
        "\n",
        "# Print the model's accuracy (R^2 score) on the training and validation sets\n",
        "print(\"Accuracy on training set: {:.8f}\".format(gbrt.score(X_train, y_train)))\n",
        "print(\"Accuracy on validation set: {:.8f}\".format(gbrt.score(X_val, y_val)))\n",
        "\n",
        "# Perform 5-fold cross-validation and return the cross-validated score of the estimator\n",
        "scores = cross_val_score(gbrt, X_train, y_train, cv=5)\n",
        "\n",
        "# Perform 5-fold cross-validation, scoring with negative mean squared error and returning both training and test scores\n",
        "scores = cross_validate(gbrt, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
        "\n",
        "# Print mean cross-validation scores for training and validation datasets, negating scores for readability\n",
        "for label_pair in [('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
        "    print('{} = {:.3f}'.format(label_pair[1], -scores[label_pair[0]].mean()))\n",
        "y_pred = gbrt.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse:.8f}\")\n",
        "# Conduct cross-validation, this time evaluating with R^2 scoring metric, and return scores for both training and test sets\n",
        "cv_results = cross_validate(gbrt, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
        "train_r2 = cv_results['train_score']  # R^2 scores on the training set\n",
        "val_r2 = cv_results['test_score']  # R^2 scores on the validation set\n",
        "\n",
        "# Calculate and print the average R^2 scores across the 5 folds for both training and validation sets\n",
        "average_train_r2 = np.mean(train_r2)\n",
        "average_val_r2 = np.mean(val_r2)\n",
        "\n",
        "gbrt_mae = mean_absolute_error(y_val, gbrt.predict(X_val))#Mean Absolute error\n",
        "\n",
        "print(f\"Average Training R^2 for GB: {average_train_r2}\")\n",
        "print(f\"Average Validation R^2 for GB: {average_val_r2}\")\n",
        "\n",
        "print(f\"MAE: {gbrt_mae:.2f}\")\n",
        "\n",
        "gbrt_feature_importances = gbrt.feature_importances_\n",
        "print(\"\\nGradient Boosting Feature Importances:\")\n",
        "for name, importance in zip(X.columns, gbrt_feature_importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yva7wJJ7E_4",
        "outputId": "545e02f6-1baf-4a9f-d28d-9da69b57084e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training set: 0.99963075\n",
            "Accuracy on validation set: 0.99804294\n",
            "train_score = 0.035\n",
            "validation_score = 0.122\n",
            "Mean Squared Error (MSE): 0.21138802\n",
            "Average Training R^2 for GB: 0.9996466995264843\n",
            "Average Validation R^2 for GB: 0.9987519990572371\n",
            "MAE: 0.30\n",
            "\n",
            "Gradient Boosting Feature Importances:\n",
            "relative compactness: 0.2911\n",
            "surface area: 0.4919\n",
            "wall area: 0.0299\n",
            "roof area: 0.0808\n",
            "overall height: 0.0151\n",
            "orientation: 0.0004\n",
            "glazing area: 0.0809\n",
            "glazing area distribution: 0.0099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # Used for numerical operations\n",
        "import pandas as pd  # Used for data manipulation and analysis\n",
        "from sklearn.model_selection import train_test_split  # Facilitates splitting datasets into training and testing sets\n",
        "from sklearn.linear_model import LinearRegression  # Regression model for predicting continuous values\n",
        "from sklearn.ensemble import RandomForestRegressor  # Ensemble model for robust regression predictions\n",
        "from sklearn.tree import DecisionTreeRegressor  # Model for regression using decision tree logic\n",
        "from sklearn.ensemble import GradientBoostingRegressor  # Ensemble model for improved predictive performance via boosting\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # Metrics for evaluating regression model performance\n",
        "from sklearn.model_selection import cross_validate, cross_val_score  # For assessing model performance using cross-validation\n",
        "from yellowbrick.datasets import load_energy  # Utility for loading example datasets\n",
        "\n",
        "# Load features and target variable from the energy dataset\n",
        "X, y = load_energy()\n",
        "\n",
        "# Split the dataset into 80% training data and 20% testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Linear Regression Model\n",
        "# -----------------------\n",
        "# Initialize the Linear Regression model\n",
        "lr = LinearRegression()\n",
        "# Train the model on the training data\n",
        "lr.fit(X_train, y_train)\n",
        "# Evaluate the model's performance on both the training and test datasets\n",
        "print(\"Linear Regression Training R^2: {:.4f}\".format(lr.score(X_train, y_train)))\n",
        "print(\"Linear Regression Test R^2: {:.4f}\".format(lr.score(X_test, y_test)))\n",
        "\n",
        "# Random Forest Regressor\n",
        "# ------------------------\n",
        "# Initialize the Random Forest model with 100 decision trees\n",
        "random_forest_model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "# Fit the Random Forest model to the training data\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "# Evaluate and print the model's accuracy on the training and test sets\n",
        "print(\"Random Forest Training R^2: {:.4f}\".format(random_forest_model.score(X_train, y_train)))\n",
        "print(\"Random Forest Test R^2: {:.4f}\".format(random_forest_model.score(X_test, y_test)))\n",
        "\n",
        "# Decision Tree Regressor\n",
        "# ------------------------\n",
        "# Initialize the Decision Tree Regressor with a maximum depth of 5\n",
        "tree = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
        "# Train the Decision Tree model\n",
        "tree.fit(X_train, y_train)\n",
        "# Output the model's performance on the training and test data\n",
        "print(\"Decision Tree Training R^2: {:.8f}\".format(tree.score(X_train, y_train)))\n",
        "print(\"Decision Tree Test R^2: {:.8f}\".format(tree.score(X_test, y_test)))\n",
        "\n",
        "# Gradient Boosting Regressor\n",
        "# ----------------------------\n",
        "# Initialize the Gradient Boosting Regressor with a depth of 5\n",
        "gbrt = GradientBoostingRegressor(max_depth=5, random_state=0)\n",
        "# Fit the model on the training data\n",
        "gbrt.fit(X_train, y_train)\n",
        "# Print the model's R^2 score for both the training and test datasets\n",
        "print(\"Gradient Boosting Training R^2: {:.6f}\".format(gbrt.score(X_train, y_train)))\n",
        "print(\"Gradient Boosting Test R^2: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
        "\n",
        "# Cross-validation for Decision Tree Regressor to evaluate model stability\n",
        "# -------------------------------------------------------------------------\n",
        "# Perform 5-fold cross-validation, returning scores for both training and testing phases\n",
        "cv_results = cross_validate(tree, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
        "# Calculate and print the average R^2 score across all folds for both training and testing\n",
        "print(f\"Average CV Training R^2 for Decision Tree: {np.mean(cv_results['train_score'])}\")\n",
        "print(f\"Average CV Test R^2 for Decision Tree: {np.mean(cv_results['test_score'])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caYHdEjC97kv",
        "outputId": "d232c2d8-c2fb-4aa1-ea76-c9a896aea55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Training R^2: 0.9177\n",
            "Linear Regression Test R^2: 0.9083\n",
            "Random Forest Training R^2: 0.9997\n",
            "Random Forest Test R^2: 0.9970\n",
            "Decision Tree Training R^2: 0.99023480\n",
            "Decision Tree Test R^2: 0.99001928\n",
            "Gradient Boosting Training R^2: 0.999553\n",
            "Gradient Boosting Test R^2: 0.998\n",
            "Average CV Training R^2 for Decision Tree: 0.9898954524705035\n",
            "Average CV Test R^2 for Decision Tree: 0.9876708795716823\n"
          ]
        }
      ]
    }
  ]
}